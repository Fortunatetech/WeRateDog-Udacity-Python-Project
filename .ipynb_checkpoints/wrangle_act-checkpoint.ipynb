{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# Project: Wrangling and Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading into pandas DataFrame the three (3) Tweeter Datasets for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading WeRateDogs Twitter archive data (twitter_archive_enhanced.csv) directly downloaded\n",
    "# into df_twitter_archive\n",
    "df_twitter_archive = pd.read_csv(\"twitter-archive-enhanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Requests library to download the tweet image prediction (image_predictions.tsv)\n",
    "\n",
    "# creating a folder to store image paths\n",
    "folder_name = 'image_path'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    \n",
    "# using requests.get to extract the image url    \n",
    "image_url= 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "with open(os.path.join(folder_name, image_url.split('/')[-1]), mode = 'wb') as file:\n",
    "        response = requests.get(image_url)\n",
    "        file.write(response.content)\n",
    "        \n",
    "# loading the image predictions into df_image_predictions \n",
    "df_image_predictions = pd.read_table('image_path/image-predictions.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In gathering the tweet_json.txt file, an alternative approach provided to gather the file was used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Tweepy library to query additional data via the Twitter API (tweet_json.txt)\n",
    "\n",
    "# creating a list to store the four variables (tweet_id, created_at, favorite_count, retweet_count)\n",
    "df_list = []\n",
    "\n",
    "# Opening the tweet_json file data and extracting the four required variables\n",
    "with open ('tweet-json.txt', 'r') as tweet_data:\n",
    "    for each_line in tweet_data:\n",
    "        data = json.loads(each_line)\n",
    "        tweet_id = data['id']\n",
    "        created_at = data['created_at']\n",
    "        favorite_count = data['favorite_count']\n",
    "        retweet_count = data['retweet_count']\n",
    "\n",
    "        # Appending to the list created \n",
    "        df_list.append({'tweet_id' : tweet_id,\n",
    "                        'created_at' : created_at,\n",
    "                        'favorite_count' : favorite_count,\n",
    "                        'retweet_count' : retweet_count})\n",
    "\n",
    "# loading the tweet_json data into df_tweet_json        \n",
    "df_tweet_json = pd.DataFrame (df_list, columns= ['tweet_id','created_at', 'favorite_count', 'retweet_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 28,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the three (3) datasets to assess it visually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the twitter_archive to assess it visually \n",
    "df_twitter_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the image_predictions to assess it visually \n",
    "df_image_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the tweet_json to assess it visually \n",
    "df_tweet_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Assessing the three (3) datasets programmatically "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing twitter_archive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the first 5 rows of twitter_archive table\n",
    "df_twitter_archive.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the last 5 rows of twitter_archive table\n",
    "df_twitter_archive.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking 10 randomly rows of twitter_archive table\n",
    "df_twitter_archive.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for more information in twitter_archive table\n",
    "df_twitter_archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values in twitter_archive table\n",
    "df_twitter_archive.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are null values in twitter_archive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicaete rows in twitter_archive table\n",
    "df_twitter_archive.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicate row in twitter_archive table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### working on name column in twitter_archive table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of dog in lower case\n",
    "df_twitter_archive['name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for names of dog in name column where the names of dog is in lower case and compairing it with list stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_names = df_twitter_archive.name\n",
    "\n",
    "# Return lowercase characters in name column\n",
    "names_with_lower_case = [lower_case for lower_case  in original_names if lower_case.islower()]\n",
    "print(names_with_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using stowords approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading stopwords and printing out the english version of stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compairing the list of names with lower case in name column in twitter_archive table and the list of stopwords (from nltk.corpus). It was observed that the following list of names [‘a’, ‘all’, ‘an’, ‘by’, ‘his’, ‘just’, ‘my’, ‘not’, ‘such’, ‘the’, ‘this’, ‘very’] are not relevant or valid in column name hence the should be removed\n",
    "\n",
    "\n",
    "Stop words are the words in a stop list (or stoplist or negative dictionary) which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for more information where in_reply_to_status_id is null\n",
    "df_twitter_archive[df_twitter_archive[\"in_reply_to_status_id\"].isnull()].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### observing expanded_urls column in df_twitter_archive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive[df_twitter_archive['expanded_urls'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### observing rating_numerator and rating_denominator column in df_twitter_archive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive['rating_numerator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive['rating_denominator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out all ratings found in each tweet in text column\n",
    "rating_ratios = []\n",
    "for rating_ratio_index in df_twitter_archive.index.tolist():\n",
    "    ratio = re.findall(r'\\d+/\\d+', df_twitter_archive.loc[rating_ratio_index, 'text'])\n",
    "    rating_ratios.append(ratio)\n",
    "    \n",
    "rating_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rating columns are found to have wrong rating ratios and some rows having more than one rating ratios. probably containing more than one Dog "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing image_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing 20 random rows in image_prediction table\n",
    "df_image_predictions.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for more information in image_prediction table\n",
    "df_image_predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicate rows in image_prediction table\n",
    "df_image_predictions.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicaete in the image_prediction table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values in image_prediction table\n",
    "df_image_predictions.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No null value in image_prediction table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observing  dog prediction column in image_predictions table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if row prediction p1_dog, p2_dog and p3_dog in image_prediction are False\n",
    "df_image_predictions.query(\"p1_dog==False and p2_dog==False and p3_dog == False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "324 rows in image_prediction table has prediction p1_dog, p2_dog and p3_dog all False. These are not valid for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing tweet_json Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the first 5 rows in  tweet_json table\n",
    "df_tweet_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for more information in tweet_json table\n",
    "df_tweet_json.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values in tweet_json table\n",
    "df_tweet_json.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicaete rows in tweet_json table\n",
    "df_tweet_json.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "1. Timestamp column in twitter_archive table should be changed to datetime. \n",
    "\n",
    "2. Names of dog like 'a', 'all', 'an', 'by', 'his', 'just', 'my', 'not', 'such', 'the', 'this', 'very' in names column in twitter_archive table should be removed and set as NaN value \n",
    "\n",
    "3. Unavailable Dog names in name column represented as none and empty spaces in dataframe should be changed to NaN to maintain consistency throughout the table.\n",
    "\n",
    "4. Rows with retweeted_status_id, retweeted_status_user_id and retweeted_status_timestamp should be removed as they are not needed for our analysis.\n",
    "\n",
    "5. Rows with in_reply_to_status_id and in_reply_to_user_id should be droped as they are redundant rows and are not needed for our analysis.\n",
    "\n",
    "6. Tweets rows without expanded_urls i.e no images, should be dropped. keep only original tweets with images.\n",
    "\n",
    "7. Wrong rating_numerator and rating_denominator\n",
    "\n",
    "8. Underscores in image prediction(p1, p2 and p3) column should be removed and casing should be in proper form. \n",
    "\n",
    "9. Prediction p1_dog, p2_dog and p3_dog all contains false in a row in the image_prediction table which are not valid for our analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 40,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Tidiness issues\n",
    "1. doggo, floofer, pupper, and puppo columns in twitter_archive should be replaced with single column (say dog_stage).\n",
    "\n",
    "2. favorite_count and retweet_count columns in tweet_json amd image_pridictions table should be merged twitter_archive table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue documentation in few sentences\n",
    "In this project I have gathered 3 datasets: twitter_archive, image_prediction and tweet_json.\n",
    "The assessment was done both visually and programmatically.\n",
    "\n",
    "While assessing the data, I observed that the entire 3 datasets has about 9 quality issues and about 2 tidiness issues.\n",
    "\n",
    "### Quality issues\n",
    "#### twitter_archive table has quality issues like:\n",
    "1. Wrong data type for the timestamp column\n",
    "2. Wrong entry of names of dog like 'a', 'all', 'an', 'by', 'his', 'just', 'my', 'not', 'such', 'the', 'this', 'very' in names column\n",
    "3. Unavailable Dog names in name column represented as none.\n",
    "4. Redundancy with retweeted_status_id, retweeted_status_user_id and retweeted_status_timestamp. \n",
    "5. Redundancy Rows with in_reply_to_status_id and in_reply_to_user_id should be droped as they are redundant rows.\n",
    "6. Tweets rows without expanded_urls i.e no images in expanded_urls column.\n",
    "7. Wrong rating_numerator and rating_denominator\n",
    "#### image_prediction table has quality issues like:\n",
    "8. validity issue in image prediction(p1, p2 and p3). \n",
    "9. prediction p1_dog, p2_dog and p3_dog all contains false in a row in the image_prediction table which are not valid for our analysis.\n",
    "\n",
    "### Tidiness issues\n",
    "#### twitter_archive table has tidiness issues like:\n",
    "1. doggo, floofer, pupper, and puppo columns in twitter_archive should be replaced with single column (say dog_stage).\n",
    "2. favorite_count and retweet_count columns in tweet_json amd image_pridictions table should be merged twitter_archive table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 32,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of original dataframe of assesed dataframe\n",
    "df_twitter_archive_clean = df_twitter_archive.copy()\n",
    "df_image_predictions_clean = df_image_predictions.copy()\n",
    "df_tweet_json_clean = df_tweet_json.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness issues\n",
    "### Issue #1: \n",
    "doggo, floofer, pupper, and puppo columns in twitter_archive should be replaced with single dog_stage column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: \n",
    "convert [\"doggo\", \"flooter\", \"pupper\", \"puppo\"] in twitter_archive columns into one 'dog_stage' column, then drop the four columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column 'dog_stage' and assigning concatenated rows in the 4 variables to it \n",
    "df_twitter_archive_clean['dog_stage'] = df_twitter_archive_clean.doggo + df_twitter_archive_clean.floofer + df_twitter_archive_clean.pupper + df_twitter_archive_clean.puppo\n",
    "\n",
    "# replacing the combined none values to empty space \n",
    "df_twitter_archive_clean['dog_stage'] = df_twitter_archive_clean['dog_stage'].str.replace(\"None\",\"\")\n",
    "\n",
    "# seperating the concatenated values in the 4 columns into separate individual values \n",
    "df_twitter_archive_clean.loc[df_twitter_archive_clean['dog_stage'] == 'doggopupper', 'dog_stage'] = 'doggo, pupper'\n",
    "df_twitter_archive_clean.loc[df_twitter_archive_clean['dog_stage'] == 'doggopuppo', 'dog_stage'] = 'doggo, puppo'\n",
    "df_twitter_archive_clean.loc[df_twitter_archive_clean['dog_stage'] == 'doggofloofer', 'dog_stage'] = 'doggo, floofer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the initial 4 columns \n",
    "df_twitter_archive_clean = df_twitter_archive_clean.drop(['doggo', 'floofer', 'pupper', 'puppo'], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean[df_twitter_archive_clean['dog_stage']!= '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean['dog_stage'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #2: \n",
    "favorite_count and retweet_count columns in tweet_json and image_pridictions table should be merged twitter_archive table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "#### Define\n",
    "merge \"favorite_count\" and \"retweet_count\" in df_tweet_json_clean and image_pridictions_clean with df_twitter_archive_clean on tweet_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the columns in tweet_json_clean table to merge\n",
    "df_tweet_json_clean = pd.DataFrame(df_tweet_json_clean, columns=[\"tweet_id\", \"favorite_count\", \"retweet_count\"])\n",
    "\n",
    "# merging the df_twitter_archive_clean with df_tweet_json_clean\n",
    "df_twitter_archive_clean = pd.merge(df_twitter_archive_clean, df_tweet_json_clean, on=['tweet_id'], how='left')\n",
    "\n",
    "# merging the df_twitter_archive_clean with df_image_predictions_clean\n",
    "df_twitter_archive_clean = pd.merge(df_twitter_archive_clean, df_image_predictions_clean, on=['tweet_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_twitter_archive_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "### Issue #1: \n",
    "Timestamp column in twitter_archive table  should be datetime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "change the timestamp column to datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean.timestamp = df_twitter_archive_clean.timestamp.astype('datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #2:\n",
    "Wrong entry of names of dog like 'a', 'all', 'an', 'by', 'his', 'just', 'my', 'not', 'such', 'the', 'this', 'very' in names column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "create a list of names to replace and use for loop to give indvidual name \"NaN\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the words in names_with_lower_case with None\n",
    "names_with_lower_case\n",
    "for word in names_with_lower_case:\n",
    "    df_twitter_archive_clean['name'].replace(word, np.NaN , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_twitter_archive_clean['name'].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #3: \n",
    "Unavailable Dog names in name column represented as none and empty spaces in dataframe should be changed to NaN to maintain consistency throughout the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "Replace the None values in the dataframe and the empty spaces in dataframe to NaN values using replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the None values with NaN \n",
    "df_twitter_archive_clean.replace('None', np.NaN, inplace = True)\n",
    "\n",
    "# empty space found in dog_stage should be replaced with NaN values\n",
    "df_twitter_archive_clean['dog_stage'].replace('', np.NaN, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #4: \n",
    "Rows with retweeted_status_id, retweeted_status_user_id and retweeted_status_timestamp should be removed as they are not needed for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "Find the index of retweets, then remove retweets rows and and drop 'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp' columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only 'retweeted_status_id' since all the rows that has retweet in 'retweeted_status_user_id' \n",
    "# and 'retweeted_status_timestamp' columns has same index with 'retweeted_status_id'\n",
    "\n",
    "retweeted_rows_index = list(df_twitter_archive_clean[df_twitter_archive_clean[\"retweeted_status_id\"].isnull()==False].index)\n",
    "\n",
    "# Dropping the rows\n",
    "df_twitter_archive_clean = df_twitter_archive_clean.drop(axis=0, index = retweeted_rows_index)\n",
    "\n",
    "#Dropping the columns \n",
    "df_twitter_archive_clean = df_twitter_archive_clean.drop(['retweeted_status_id',\n",
    "                                                          'retweeted_status_user_id',\n",
    "                                                          'retweeted_status_timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for retweet in retweeted_rows_index:\n",
    "    if retweet in list(df_twitter_archive_clean.index):\n",
    "        print('Found a retweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #5: \n",
    "Rows with in_reply_to_status_id and in_reply_to_user_id should be droped as they are redundant rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "Find and remove the index rows of reply tweets 'in_reply_to_user_id' and 'in_reply_to_status_id' and drop the two columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only \"in_reply_to_status_id\" since all the rows that has replied tweet in \n",
    "# 'in_reply_to_user_id' and 'in_reply_to_status_id' columns has same index with \"in_reply_to_status_id\"\n",
    "\n",
    "replied_index_rows = list(df_twitter_archive_clean[df_twitter_archive_clean[\"in_reply_to_status_id\"].isnull()==False].index)\n",
    "\n",
    "# Dropping replied_index_rows\n",
    "df_twitter_archive_clean = df_twitter_archive_clean.drop(axis=0, index = replied_index_rows)\n",
    "\n",
    "# Dropping 'in_reply_to_user_id' and 'in_reply_to_status_id' columns \n",
    "df_twitter_archive_clean = df_twitter_archive_clean.drop(['in_reply_to_user_id', \n",
    "                                                          'in_reply_to_status_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reply in replied_index_rows:\n",
    "    if reply in list(df_twitter_archive_clean.index):\n",
    "        print('Found a reply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #6: \n",
    "Tweets rows without expanded_urls i.e no images should be dropped, keep only original tweets with images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define \n",
    "look for row index without expanded_url and drop them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: row index without expanded_url has been reduced to 3 rows as result of the cleaning processes that have be done on the dataframe earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the row index without expanded_urls in rows_without_expanded_urls\n",
    "rows_without_expanded_urls = list(df_twitter_archive_clean[df_twitter_archive_clean['expanded_urls'].isnull()].index)\n",
    "\n",
    "# Dropping replied_index_rows\n",
    "df_twitter_archive_clean = df_twitter_archive_clean.drop(axis=0, index = rows_without_expanded_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #7: \n",
    "Wrong rating_numerator and rating_denominator ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define \n",
    "create a variable where we are going to save the entire ratios extracted from text column in the twitter_archive column. \n",
    "\n",
    "Then remove the ratios with more than one rating ratio and then include the single rating ratios in the cleaned twitter_archive dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list where to save  rating ratios extracted in text column\n",
    "rating_ratios = []\n",
    "\n",
    "#looping through to extract each rating ratio\n",
    "for rating_ratio_index in df_twitter_archive_clean.index.tolist():\n",
    "    ratio = re.findall(r'\\d+/\\d+', df_twitter_archive_clean.loc[rating_ratio_index, 'text'])\n",
    "    \n",
    "    # Appending extracted rating ratios in created list\n",
    "    rating_ratios.append(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the rating ratios \n",
    "rating_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for the total number of rating ratios extracted\n",
    "len(rating_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column in twitter_archive dataFrame and assigning it to list of ratios extracted\n",
    "df_twitter_archive_clean['rating'] = rating_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list for rating ratios more than one \n",
    "indexes_other_than_1_ratio = []\n",
    "\n",
    "# looping through to extract the rating ratios other than one\n",
    "for index_ratio in df_twitter_archive_clean.index.tolist():\n",
    "    if len(df_twitter_archive_clean.loc[index_ratio, 'rating']) > 1:\n",
    "        \n",
    "        # Appending extracted rating ratios other than one in created list\n",
    "        indexes_other_than_1_ratio.append(index_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the rating ratios other than one \n",
    "df_twitter_archive_clean.loc[indexes_other_than_1_ratio, \n",
    "                          ['text', 'rating_numerator', 'rating_denominator','rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indexes_other_than_1_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only the rows that has only one rating ratios \n",
    "df_twitter_archive_clean = df_twitter_archive_clean[df_twitter_archive_clean.index.isin(indexes_other_than_1_ratio)== False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the 'rating' column in the twitter_archive table\n",
    "df_twitter_archive_clean = df_twitter_archive_clean.drop('rating', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #8: \n",
    "prediction p1_dog, p2_dog and p3_dog all contains false in a row in the image_prediction table which are not valid for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "find rows that have three false and drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_3 = list(df_image_predictions_clean.query(\"p1_dog==False and p2_dog==False and p3_dog == False\").index)\n",
    "df_image_predictions_clean= df_image_predictions_clean.drop(index=false_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_predictions_clean.query(\"p1_dog==False and p2_dog==False and p3_dog == False\").index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_predictions_clean.query(\"p1_dog==False and p2_dog==False and p3_dog == False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_predictions_clean.query(\"p1_dog==False and p2_dog==False and p3_dog == False\").any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "### Issue #9: \n",
    "underscores in image prediction(p1, p2 and p3) column should be removed and casing should be in proper form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define \n",
    "Remove underscores and change image descriptions to proper case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_predictions_clean['p1'] = df_image_predictions_clean['p1'].str.replace('_', ' ')\n",
    "df_image_predictions_clean['p2'] = df_image_predictions_clean['p2'].str.replace('_', ' ')\n",
    "df_image_predictions_clean['p3'] = df_image_predictions_clean['p3'].str.replace('_', ' ')\n",
    "\n",
    "df_image_predictions_clean['p1'] = df_image_predictions_clean['p1'].str.title()\n",
    "df_image_predictions_clean['p2'] = df_image_predictions_clean['p2'].str.title()\n",
    "df_image_predictions_clean['p3'] = df_image_predictions_clean['p3'].str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_predictions_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the gathered, assessed, and cleaned twitter_archive datasets to a csv file format\n",
    "df_twitter_archive_clean.to_csv('twitter_archive_master.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing and Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the master dataframe\n",
    "df_master = pd.read_csv('twitter_archive_master.csv')\n",
    "df_master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyzing of Dogs with top favourite count  with their corresponding tweet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using groupby to group tweet_id, name and with favorite_count\n",
    "dog_with_fav_count = df_master.groupby(['tweet_id', 'name'])['favorite_count'].max()\n",
    "\n",
    "# sorting the dogs favorite_count in descending order and using head() to grap the first 10\n",
    "dog_with_fav_count_sort = dog_with_fav_count.sort_values(ascending = False).head(10)\n",
    "dog_with_fav_count_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The display above shows the top 10 Dogs with highest favorite_count along side with their names and tweet_id.\n",
    "\n",
    "The above analysis shows that 3 dog with name **Duddles**, **Stephan** and **Jamesy** with tweet_id **879415818425184262**, **807106840509214720** and **866450705531457537** respectively has a really close favorite_count.\n",
    "\n",
    "* Duddles has the highest favorite_count of **107956.0** \n",
    "* Stephan has favorite_count of **107015.0** \n",
    "* Jamesy has favorite_count of **106827.0** \n",
    "\n",
    "But based on my analysis, **Duddles** with tweet_id **879415818425184262** has the highest favorite_count of **107956.0** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyzing Dog stages with top favourite count  and retweet_count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_stage_max = df_master.groupby('dog_stage').max()\n",
    "dog_stage_max = dog_stage_max[['favorite_count', 'retweet_count']]\n",
    "dog_stage_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The display above shows the maximum favorite_count and retweet_count of dog stages.\n",
    "\n",
    "Analysis shows that dog stage with name **doggo** and **puppo** has a really close favorite_count but huge difference in retweet_count.\n",
    "\n",
    "doggo has the highest favorite_count of **131075.0** and retweet_count **79515.0**\n",
    "\n",
    "puppo has favorite_count of **132810.0** and retweet_count **48265.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyzing Year with the highest favorite count and the Dog who has the win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master['timestamp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting year of highest favorite_count with ther corresponding dog name\n",
    "year_with_hihest_fav = df_master.groupby(['timestamp', 'name'])['favorite_count'].max()\n",
    "year_with_hihest_fav = year_with_hihest_fav.sort_values(ascending = False).head(1)\n",
    "year_with_hihest_fav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The display above shows the top year of highest favorite_count with ther corresponding dog name.\n",
    "\n",
    "The above analysis shows that the year 2017 has highest favorite_count of **107956.0** Dog with name **Duddles**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "1. Dogs with highest favourite count\n",
    "\n",
    "2. Dog stage with highest favourite count and retweet count \n",
    "\n",
    "3. Year with the highest favorite count and the Dog who has the win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Top 10 Dogs with highest favourite count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representing the first insigth with visuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using groupby to group name and with favorite_count\n",
    "dog_with_fav_count = df_master.groupby(['name'])['favorite_count'].max()\n",
    "\n",
    "# sorting the dogs favorite_count in descending order and using head() to grap the first 10\n",
    "dog_with_fav_count_sort = dog_with_fav_count.sort_values(ascending = False).head(10)\n",
    "\n",
    "# plotting a bar chart representation \n",
    "dog_with_fav_count_sort.plot(kind = 'bar', legend = 'favorite_count', figsize = (8,8))\n",
    "\n",
    "plt.title('Top 10 Dogs with highest favorite')\n",
    "plt.xlabel('name', fontsize=13)\n",
    "plt.ylabel('favorite_count', fontsize=13)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The display above shows the top 10 Dogs with highest favorite_count along side with their names and tweet_id.\n",
    "\n",
    "The above analysis shows that Dog with name **Duddles**, tweet_id  879415818425184262 has the highest favorite_count of 107956.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dog_stage with highest favourite_count and retweet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorizing the variables to plot\n",
    "dog_stage_max_viz = dog_stage_max[['favorite_count', 'retweet_count']].plot(kind='bar', title =\" favorite_count and retweet_count of dog's stage\",\n",
    "                   figsize=(8,6), legend=True, fontsize=13)\n",
    "\n",
    "# setting up the labels and the font size \n",
    "dog_stage_max_viz.set_xlabel(\"Dog stage\", fontsize=13)\n",
    "dog_stage_max_viz.set_ylabel(\"Count\", fontsize=13)\n",
    "\n",
    "plt.legend(fontsize = 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The display above shows the maximum favorite_count and retweet_count of dog stages.\n",
    "\n",
    "Analysis shows that dog stage with name **doggo** and **puppo** has a really close favorite_count but huge difference in retweet_count.\n",
    "\n",
    "doggo has the highest favorite_count of **131075.0** and retweet_count **79515.0**\n",
    "\n",
    "puppo has favorite_count of **132810.0** and retweet_count **48265.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Year with the highest favorite count and the Dog who had the win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_fav_highest_year = df_master.groupby(['timestamp', 'name'])['favorite_count'].max()\n",
    "top_20_fav_highest_year = top_20_fav_highest_year.sort_values(ascending = False).head(20)\n",
    "top_20_fav_highest_year.plot(kind = 'bar', figsize = (10,8), legend = 'favorite_count')\n",
    "\n",
    "plt.title('Top 20 year of highest favorite_count of dogs with thier corresponding names')\n",
    "plt.xlabel('timestamp', fontsize=13)\n",
    "plt.ylabel('favorite count', fontsize=13)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The display above shows 20 top year of highest favorite_count with ther corresponding dog name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data used for my analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**twitter_archive_master.csv**"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
